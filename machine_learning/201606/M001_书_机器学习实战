001 本书结构
    第一部分    分类
        第01章  机器学习基础
        第02章  k-进邻算法
        第03章  决策树
        第04章  基于概率论的分类方法：朴素贝叶斯
        第05章  Logistic回归
        第06章  支持向量机
        弟07章  利用AdaBoost元算法提高分类性能
    第二部分    利用回归预测数值型数据
        第08章  预测数值型数据:回归
        第09章  树回归
    第三部分    无监督学习
        第10章  利用K-均值聚类算法对未标注数据分组
        第11章  使用Apriori算法进行关联分析
        第12章  使用FP-growth算法来高效发现频繁项集
    第四部分    其他工具
        第13章  利用PCA来简化数据
        第14章  利用SVD简化数据
        第15章  大数据与MapReduce
002 什么是机器学习
    1,  机器学习就是把无序的数据转换成有用的信息。
    2,  机器学习的主要任务是分类
    3,  要使用某个机器学习算法进行分析
            首先需要做的是算法训练，即学习如何分类。通常为算法输入大
        量已分类数据作为算法的训练集训练集是用于训练机器学习算法的数
        据样本集合
    4,  为了测试机器学习的效果，通常使用两套独立的样本集：训练数据和
        测试数据。
    5,  机器学习的另一项任务是回归(前面提到的任务是；分类)回归主要用
        于预测数据，回归的例子---数据拟合曲线
    6,  分类和回归属于监督学习，这类算法必须知道预测什么，即目标变量
        的分类信息。
    7,  与监督学习相对应的是无监督学习，此时数据没有类别信息，也不会
        给定目标值。
            在无监督学习中，将数据集合分成由类似的对象组成的多个类的
        过程被称为聚类；将寻找描述数据统计值得过程称之为密度估计。此
        外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维
        或三维图形更加直观地展示数据信息。
    8,  机器学习的主要任务以及解决相应问题的算法。
                监督学习的用途
        k-近邻算法                  线性回归
        朴素贝叶斯算法              局部加权线性回归
        支持向量机                  Ridge回归
        决策树                      lasso最小回归系数估计
                无监督学习的用途
        K-均值                      最大期望算法
        DBSCAN                      Parzen窗设计
003 如何选择合适的算法
    1,  考虑问题一、使用机器学习算法的目的，想要算法完成何种任务，比
        如是预测明天下雨的概率还是对投票者按照兴趣分组
    2,  考虑问题二、需要分析或收集的数据是什么
    3,  如果想要预测目标变量的值，则可以选择监督学习算法；否则可以选
        择无监督学习算法。确定选择监督学习算法之后，需要进一步确定目
        标变量类型，如果目标变量是离散型，如是/否、1/2/3、A/B/C或者
        红/黄/黑等，则可以选择分类器算法；如果目标变量是连续型的数值
        ，如0.00~100.00、-999~999或者+无穷大 -无穷大等，则需要选择回
        归算法。
    4,  如果不想预测目标变量的值，则可以选择无监督学习算法。进一步分
        析是否需要将数据划分为离散的组。如果这是唯一的需求，则使用聚
        类算法；还需要估计数据与每个分组的相似程度，则需要使用密度估
        计算法。
    5,  其次需要考虑的是数据问题。对实际数据了解越充分，越容易创建符
        合实际需求的应用程序。主要应该了解数据的以下特性：特征值是离
        散型变量还是连续型变量，特征值中是否存在缺失的值，何种原因造
        成缺失值，数据中是否存在异常值，某个特征发生的频率如何(是否罕
        见得如同大海捞针)，等等。充分了解上面提到的这些数据特性可以缩
        短选择机器学习算法的时间。
    6,  我们只能在一定程度上缩小算法的选择范围，一般不存在最好的算法
        或者可以给出最好结果的算法，同时还要尝试不同算法的执行效果。
        对于所选的每种算法，都可以使用其他的机器学习技术来改进其性能
        。在处理输入数据之后，两个算法的相对性能也可能会发生变化。
    7,  机器学习算法虽然各部相同，但是使用算法创建应用程序的步骤却基
        本类似。
004 开发机器学习应用程序的步骤
    1,  收集数据
        爬虫，RSS返回，设备发送过来的实测数据(风速、血糖等)
    2,  准备输入数据
        得到数据之后，还必须确保数据格式符合要求。
    3,  分析输入数据
        分析以前得到的数据。
    4,  训练算法
        将前两步得到的格式化数据输入到算法，从中抽取知识或信息。这里
        得到的知识需要存储为计算机可以处理的格式。
        如果使用无监督学习算法，由于不存在目标变量值，故而也不需要训
        练算法， 所有与算法相关的内容都集中在第5步
    5,  测试算法
        实际使用第4步机器学习得到的知识信息。为了评估算法，必须测试算
        法的工作效果。对于监督学习，必须一致用于评估算法的目标变量值
        ；对于无监督学习，也必须用其他的评测手段来检测算法的成功率。
        无论哪种清醒，如果不满意算法的输出结果，则可以回到第4步，改进
        并加以测试。问题常常会跟数据的收集和准备有关，这时必须回到第1
        步重新开始。
    6,  使用算法
        将机器学习算法转换为应用程序，执行实际任务，以检验上诉步骤是
        否可以在实际环境中正常工作。此时，如果碰到新的问题，同样需要
        重复执行上诉的步骤。
005 Python的特点
    1,  Python的语法清晰
    2,  易于操作纯文本文件
    3,  使用广泛，存在大量的开发文档
    4,  Python可调用C编译的代码
006 NumPy函数库基础
    1,  >>> from numpy import *
    2,  NumPy矩阵与数组的区别
        NumPy函数库中存在两种不同的数据类型(矩阵matrix和数组array),
        都可以用于处理行列表示的数字元素。虽然它们看起来相似，但是在
        这两个数据类型上执行相同的数学运算可能得到不同的结果，其中
        NumPy函数库中的matrix与MATLAB中matrices等价。
    3,  调用mat()函数可以将数组转化为矩阵，输入下述命令
        >>> randMat=mat(random.rand(4,4))
        由于使用随机函数产生举证，不同计算机上输出的值可能略有不同
        >>> randMat.I
        .I操作符实现了矩阵的逆运算
        >>> invRandMat = randMat.I
        接着执行举证惩罚
        >>> randMat*invRandMat
        结果应该是单位矩阵，实际矩阵中留下许多非常小的元素
007 k-近邻算法本章内容：
    1,  k-近邻分类算法
    2,  从文本文件中解析和导入数据
    3,  使用Matplotlib创建扩散图
    4,  归一化数值
008 k-近邻算法(kNN)概述
    1,  优点：精度高、对异常值不敏感、无数据输入假定
        缺点：计算复杂度高、空间复杂度高
        适用数据范围：数值型和标称型
    2,  k-近邻算法(kNN)的工作原理是： 存在一个数据集合，也称作训练样本
        集，并且样本集中每个数据都存在标签，即我们知道样本集合中每一数
        据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个
        特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征
        最相似数据最近邻的分类标签。
    3,  一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近
        邻算法中k个出处，通常k是不大于20的整数。最后，选择k个最相似数
        据中出现次数最多的分类，作为新数据的分类。
    4,  k-近邻算法的一般流程
        1,  收集数据：可以使用任何方法
        2,  准备数据：距离计算所需要的数值，最好是结构化的数据格式
        3,  分析数据：可以使用任何方法
        4,  训练算法：此步骤不使用于k-近邻算法
        5,  测试算法：计算错误率
        6,  使用算法：首先需要输入样本数据和结构化的输出结果，然后运行
                      k-近邻算法判定输入数据分别属于哪个类，最后应用对
                      计算出的分类执行后续的处理
    5,  准备：使用Python导入数据
        1,  创建名为kNN.py的Python模块
        2,  构造完整的k-近邻算法之前，需要准备一些基本的通用函数
            from numpy import *
            import operator
            def createDataset():
                group=array([[1.0,1.1],[1.0,1.0],[0.0,0.0],[0.0,0.1]])
                labels=['A','A','B','B']
                return group,labels
            上面的代码中，导入了两个模块；科学计算包NumPy和运算符模块
        3,  为了方便使用createDataSet()函数，它创建数据集和标签，然后
            依次执行以下步骤：保存kNN.py文件，改变当前路径到存储kNN.py
            文件的位置，打开Python开发环境。
        4,  Python交互式开发
            >>> import kNN
            上述命令导入kNN模块。 
            >>> group.label=kNN.createDataSet()
            上面命令创建了变量group和labels
        5,  通常人适合处理三维以下的事务，因此为了简单地实现数据的可视
            化，对于每个数据点我们通常只使用两个特征值。
    6,  从文本文件中解析数据
        1,  伪代码：
            (1) 计算一直类别数据集中的点与当前点之间的距离
            (2) 按照距离递增次序排序
            (3) 选取与当前点距离最小的k个点
            (4) 确定前k个点所在类别的出现频率
            (5) 返回前k个点出现频率最高的类别作为当前点的预测分类
        2,  Python程序
            def classify0(inX, dataSet, labels, k):
                dataSetSize = dataSet.shape[0]
                diffMat = tile(inX, (dataSetSize,1)) - dataSet
                sqDiffMat = diffMat**2
                sqDistances = sqDiffMat.sum(axis=1)
                distances = sqDistances**0.5
                sortedDistIndicies = distances.argsort()     
                classCount={}          
                for i in range(k):
                    voteIlabel = labels[sortedDistIndicies[i]]
                    classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
                sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
                return sortedClassCount[0][0]
            
            def createDataSet():
                group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])
                labels = ['A','A','B','B']
                return group, labels
            
            def file2matrix(filename):
                fr = open(filename)
                numberOfLines = len(fr.readlines())         #get the number of lines in the file
                returnMat = zeros((numberOfLines,3))        #prepare matrix to return
                classLabelVector = []                       #prepare labels return   
                fr = open(filename)
                index = 0
                for line in fr.readlines():
                    line = line.strip()
                    listFromLine = line.split('\t')
                    returnMat[index,:] = listFromLine[0:3]
                    classLabelVector.append(int(listFromLine[-1]))
                    index += 1
                return returnMat,classLabelVector
                
            def autoNorm(dataSet):
                minVals = dataSet.min(0)
                maxVals = dataSet.max(0)
                ranges = maxVals - minVals
                normDataSet = zeros(shape(dataSet))
                m = dataSet.shape[0]
                normDataSet = dataSet - tile(minVals, (m,1))
                normDataSet = normDataSet/tile(ranges, (m,1))   #element wise divide
                return normDataSet, ranges, minVals
               
            def datingClassTest():
                hoRatio = 0.50      #hold out 10%
                datingDataMat,datingLabels = file2matrix('datingTestSet2.txt')       #load data setfrom file
                normMat, ranges, minVals = autoNorm(datingDataMat)
                m = normMat.shape[0]
                numTestVecs = int(m*hoRatio)
                errorCount = 0.0
                for i in range(numTestVecs):
                    classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3)
                    print "the classifier came back with: %d, the real answer is: %d" % (classifierResult, datingLabels[i])
                    if (classifierResult != datingLabels[i]): errorCount += 1.0
                print "the total error rate is: %f" % (errorCount/float(numTestVecs))
                print errorCount
                
            def img2vector(filename):
                returnVect = zeros((1,1024))
                fr = open(filename)
                for i in range(32):
                    lineStr = fr.readline()
                    for j in range(32):
                        returnVect[0,32*i+j] = int(lineStr[j])
                return returnVect
            
            def handwritingClassTest():
                hwLabels = []
                trainingFileList = listdir('trainingDigits')           #load the training set
                m = len(trainingFileList)
                trainingMat = zeros((m,1024))
                for i in range(m):
                    fileNameStr = trainingFileList[i]
                    fileStr = fileNameStr.split('.')[0]     #take off .txt
                    classNumStr = int(fileStr.split('_')[0])
                    hwLabels.append(classNumStr)
                    trainingMat[i,:] = img2vector('trainingDigits/%s' % fileNameStr)
                testFileList = listdir('testDigits')        #iterate through the test set
                errorCount = 0.0
                mTest = len(testFileList)
                for i in range(mTest):
                    fileNameStr = testFileList[i]
                    fileStr = fileNameStr.split('.')[0]     #take off .txt
                    classNumStr = int(fileStr.split('_')[0])
                    vectorUnderTest = img2vector('testDigits/%s' % fileNameStr)
                    classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)
                    print "the classifier came back with: %d, the real answer is: %d" % (classifierResult, classNumStr)
                    if (classifierResult != classNumStr): errorCount += 1.0
                print "\nthe total number of errors is: %d" % errorCount
                print "\nthe total error rate is: %f" % (errorCount/float(mTest))
                            
        3， classify0()函数有4个输入参数：用于分类的输入向量inX，输
            入的训练样本集为dataSet，标签向量为labels，最后的参数k表
            示选择最近邻居的数目，其中标签向量的元素数目和矩阵dataSet
            的行数相同。如果数据集存在4个特征值，则(1,0,0,1)与(7,6,9,
            4)之间的距离计算为sqrt((7-1)*(7-1)+(6-0)*(6-0)+(9-0)*(9-0
            )+(4-1)*(4-1))。计算完所有点之间的距离后，可以对数据从小
            到大的次序排序。然后，确定前k个距离最小元素所在的主要分类
            。最后将classCount字典分解为元祖列表，然后使用程序第二行
            导入运算模块的itemgetter方法，按照第二个元素的次序对元祖进
            行排序。此处的排序为逆序，即按照从最大到最小次序排序，最后
            返回发生频率最高的元素标签。
            为了预测数据所在分类，在Python提示符中输入下列命令：
            >>> kNN.classify0([0,0], group, labels, 3)
            输出结果应该是B,可以改变输入[0,0]为其他值，测试程序的运行
            结果。到目前为止，我们已经构造第一个分类器，使用这个分类器
            可以完成很多分类任务。
    7,  如何测试分类器
        分类器的错误率：出错次数/总次数。错误率是常用的评估方法，主要
        用于评估分类器在某个数据集上的执行效果。完美分类器的错误率为0
        ，最差分类器的错误率是1.0。
    8,  准备数据：从文本文件中解析数据
        收集了非常多的约会数据，数据存放在datingTestSet.txt中，每个样
        本数据占据一行，总共1000行。样本主要包含以下3个特征：
            a,  每年获得的飞行常客里程数
            b,  看视频玩游戏所消耗时间百分比
            c,  每周消费的冰淇淋公升数
        Python处理文本文件的步骤
            a,  打开文件，得到文件的行数
            b,  创建以零填充的矩阵NumPy
            c,  循环处理文件中的每行数据，首先使用函数line.strip()截
                取掉所有的回车字符，然后使用tab字符\t将上一步得到的整
                行数据分割成一个元素列表。
            d,  选取前3个元素，将它们存储到特征矩阵中。Python可以使用
                索引值-1表示列表中的最后一列元素。可以非常方便讲列表的
                最后一列存储到classLabelVector中。
            >>> reload(kNN)
            >>> datingDataMat, datingLabelas = kNN.file2matrix("dating
                TestSet.txt")
    9,  分析数据：使用Matplotlib创建散点图
        a,  使用Matplotlib制作原始数据的散点图
            >>> import matplotlib
            >>> import matplotlib.pyplot as plt
            >>> fig = plt.figure()
            >>> ax = fig.add_subplot(111)
            >>> ax.scatter(datingDataMat[:,1],datingDataMat[:,2])
            >>> plt.show()
        b,  由于没有使用样本分类的特征值，很难看到有用的数据模式信息
            。一般使用色彩或者其他标记号标记不同分类。Matplotlib库提供
            的scatter函数支持个性化。
            >>> ax.scatter(datingDataMat[:,1],datingDataMat[:,2], 15.0
                *array(datingLabels), 15.0*array(datingLabels))
    10, 准备数据：归一化数值
        不同数据，单位不同，必须要进行归一化，才能保证起同等效应。
        newvalue = (oldValue - min)/(max - min)
        在kNN.py中增加一个新的函数autoNorm()，完成归一化
        def autoNorm(dataSet):
            minVals = dataSet.min(0)
            maxVals = dataSet.max(0)
            ranges = maxVals - minVals
            normDataSet = zeros(shape(dataSet))
            m = dataSet.shap[0]
            normDataSet = dataSet - tile(minVals, (m, 1))
            normDataSet = normDataSet/tile(ranges, (m, 1))
            return normDataSet, ranges, minVals
        在函数autoNorm()中，将每列的最小值放在变量minVals中，将最大值
        放在变量maxVals中，其中dataSet.min(0)中的参数0使得函数可以从
        列中选取最小值，而不是选取当前行的最小值。
        对于某些数值处理软件包，/可能意味着举证除法；但在NumPy库中，
        矩阵除法需要使用函数linalg.solve(matA,matB)。
        在Python命令提示符下，重新加载kNN.py模块，执行autoNorm函数，
        检测函数的执行结果：
        >>> reload(kNN)
        >>> normMat, range, minVals = kNN.autoNorm(datringDataMat)
        >>> normMat 就会输出对应矩阵
        >>> range   就会输出矩阵
        >>> minVals 就会输出矩阵
    11, 测试算法：作为完整程序验证分类器
        分类器针对约会网站的测试代码
        def datingClassTest():
            hoRatio = 0.10
            datingDataMat.datingLabels = file2matrix("datingTestSet.txt")
            normMat, ranges, minVals = autoNorm(datingDataMat)
            m = normMat.shape[0]
            numTestVecs = int(m*hoRatio)
            errorCount = 0.0
            for i in range(numTestVecs):
                calssifiexResult = classify0(normMat[i,:], normMat[numTestVecs:m,:],datingLabels(numTestVecs:m), 3)
                print "the classifier came back with: %d, the real answer is : %d" % (classifierResult, datingLables[i])
                if (classifierResult != datingLabels[i]) : errorCout += 1.0
            print "the total error ratoe is: %f" %(errorCount/float(numTestVecs))
        得到的结果是：处理约会数据集的错误率是2.4%，这是一个非常不错的
        结果。我们可以改变函数datingClassTest内变量hoRatio和变量k的值
        ，检测错误率是否随着变量值的变化而增加，依赖分类算法、数据集和
        程序设置，分类器的输出结果有很大的不同。 
    12, 实例：手写识别系统的步骤
        1,  收集数据：提供文本文件
        2,  准备数据：编写函数classify0()，将图像格式转换为分类器使用
            的list格式。
        3,  分析数据：在Python命令提示符中检查数据，确保它符合要求。
        4,  训练算法：此步骤不使用于k-近邻算法
        5,  测试算法：编写函数使用提供而的部分数据集作为测试样本，测试
            样本与非测试样本的区别在于测试样本是已经完成分类的数据，如
            果预测分类与实际类别不同，则标记为一个错误。
        6,  使用算法：本例没有完成此步骤，从图像中提取数字，并完成数字
            识别，美国的邮件分拣系统就是一个实际运行的类似系统。
    13, 手写识别系统编写代码一:将图像格式化处理为一个向量。我们将一个
        32*32二进制图像矩阵转换为1*1024的向量，这样前两节使用的分类器
        就可以处理数字图像信息了。看代码
        def img2vector(filename):
            returnVect = zeros((1,1024))
            fx = open(filename)
            for i in range(32):
                lineStr = fr.readline()
                for j in range(32):
                    returnVect[0, 32*i +j] = int(lineStr[j])
            return returnVect
        将上诉代码输入到kNN.py文件中，在Python命令行中输入下列命令测试
        img2vector函数，然后与文本编辑器打开的文件进行比较：
        >>> testVector = kNN.img2vector('testDigits/0_13.txt')
        >>> testVector[0,0:31]
        >>> testVector[0,32,63]
    14, 手写识别系统编写代码二：测试算法，使用k-近邻算法识别手写数字
        必须确保将from os import listdir写入文件的起始部分，这段代码
        的主要功能是从os模块中导入函数listdir，它可以列出给定目录的文
        件名。
        def handwritingClassTest():
            hsLabels = []
            trainingFileList = listdir('trainingDigits')
            m = len(trainingFileList)
            trainingMat = zeros((m,1024))
            for i in range(m):
                fileNameStr = trainingFileList[i]
                fileStr = fileNameStr.split('.')[0]
                calssNumStr = int(fileStr.split('_')[0])
                hwLabels.append(classNumStr)
                trainingMat[i,:] = img2vector('trainingDigits/%s'
                %fileNameStr)
            testFileList = listdir('testDigits')
            errorCount = 0.0
            mTest= len(testFileList)
            for i in range(mTest)
                fileNameStr = testFileList[i]
                fileStr = fileNameStr.split('.')[0]
                calssNumStr = img2vector('testDigits/%s'%fileNameStr)
                calssifierResult = calssify0(vectorUnderTest, 
                    \trainingMat, hwLabels, 3)
                print "the classifier came back with : %d, the real
                    answer is : %d " %(classifierResult, classNumStr)
                if (calssifierResult != classNumStr) : errorcount+=1.0
            print "\nthe total number of errors is: %d"%errorCount
            print "\nthe total error rato is: %f"%(errorCount/
                float(nTest))
        到此，k-近邻算法介绍完成了。 
                
            
