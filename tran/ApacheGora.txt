2010年9月22日，Gora进入到Apache孵化器
Apache中与Hadoop相关的项目，它们在百度中的检索数:
    Apache Spark:       3,100,000
    Apache HBase:       2,050,000
    Apache Hive:        1,970,000
    Apache Pig:         1,640,000
    Apache Cassandra:   1,100,000
    Apache Mahout:      577,000
    Apache Avro:        376,000
    Apache Ambari:      215,000
    Apache Tez:         183,000
    Apache Gora:        59,400
可以明显看到，Apache Gora的中文检索数最少。作为Apache中与Hadoop相关的一个框架，我们有必要对Apache Gora有一个技术上的学习。

Apache Gora Project 什么是Apache Gora Project
The Apache Gora open source framework provides an in-memory data model and persistence for big data. Gora supports persisting to column stores, key value stores, document stores and RDBMSs, and analyzing the data with extensive Apache Hadoop MapReduce support.
Apache Gora是一个开源大数据持久化框架并提供内存数据模型。Gora提供持久化到列存储(column stores)，键值存储(key value stores)，非表结构存储(document stores)和关系型数据库管理系统(RDBMSs)。Gora通过Hadoop MapReduce扩展来分析数据。
Why Gora? 为什么选择Gora
Although there are various excellent ORM frameworks for relational databases, data modeling in NoSQL data stores differ profoundly from their relational cousins. Moreover, data-model agnostic frameworks such as JDO are not sufficient for use cases, where one needs to use the full power of the data models in column stores. Gora fills this gap by giving the user an easy-to-use ORM framework with data store specific mappings and built in Apache Hadoop support.
尽管关系型数据库有一系列优秀的ORM框架，NoSQL数据仓库的数据建模与关系型数据库有很大不同。此外，如果想要充分利用列存储的数据模型，以JDO为代表的不依赖具体实现的数据模型是远远不够的。Gora通过提供易于使用的ORM框架和数据仓库具体化映射和自带的Apache Hadoop支持，填补了这个空缺。
The overall goal for Gora is to become the standard data representation and persistence framework for big data. The roadmap of Gora can be grouped as follows.
    Data Persistence : Persisting objects to Column stores such as HBase, Cassandra, Hypertable; key-value stores such as Voldermort, Redis, etc; SQL databases, such as MySQL, HSQLDB, flat files in local file system or Hadoop HDFS.
    Data Access : An easy to use Java-friendly common API for accessing the data regardless of its location.
    Indexing : Persisting objects to Lucene and Solr indexes, accessing/querying the data with Gora API.
    Analysis : Accesing the data and making analysis through adapters for Apache Pig, Apache Hive and Cascading
    MapReduce support : Out-of-the-box and extensive MapReduce (Apache Hadoop) support for data in the data store.
Gora的总体目标是成为大数据中数据呈现和持久化的标准。Gora的路线图以如下展示：
    数据持久化：持久化对象到列存储(HBase,Cassandra,Hypertable)，键值存储(Voldermort,Redis)，SQL数据库(MySQL)，HSQLDB(没有关系结构的文件，在本地文件系统或HDFS)
    数据存取：一个易于使用的通用API,Java友好，用于存取数据而不考虑数据的实际存储位置
    数据检索：持久化对象到Lucene和Solr检索中，使用Gora API来存取和检索数据
    数据分析：通过提供给Apache Pig,Apache Hive和Cascading的适配器，来存取数据和做数据分析
    MapReduce支持：成熟易用有外延的MapReduce给数据仓库中的数据提供支持。
Background
ORM stands for Object Relation Mapping. It is a technology which abstacts the persistency layer (mostly Relational Databases) so that plain domain level objects can be used, without the cumbersome effort to save/load the data to and from the database. Gora differs from current solutions in that:
    1, Gora is specially focussed at NoSQL data stores, but also has limited support for SQL databases.
    2, The main use case for Gora is to access/analyze big data using Hadoop.
    3, Gora uses Avro for bean definition, not byte code enhancement or annotations.
    4, Object-to-data store mappings are backend specific, so that full data model can be utilized.
    5, Gora is simple since it ignores complex SQL mappings.
    6, Gora will support persistence, indexing and anaysis of data, using Pig, Lucene, Hive, etc.
    For the latest information about Gora, please visit our website at:http://gora.apache.org 
ORM是对象关系映射的简称。ORM技术抽象出持久化层，可以减少对数据库存取的繁重过程。Gora和现有解决方案的区别在于：
    1,Gora专门聚焦在NoSQL数据仓库，同时对SQL数据库有有限的支持。
    2,Gora的主要使用场景：使用Hadoop存取和分析大数据
    3,Gora使用Avro作bean定义，而不是字节码或注释
    4,Object-to-data存储映射是后端具体化的，那样的话就能够实现full-data-model
    5,Gora忽略复杂的SQL映射，故Gora是简单的
    6,Gora将会支持使用Pig,Lucene,Hive等来支持持久化，检索和数据分析
    获取Gora的最新信息，访问http://gora.apache.org
What Platform(s) does Gora work on? Gora运行在哪些平台
Gora builds nightly on Ubuntu.
The software has been tested and verified to run on the following platforms:
    Mac OSX 10.9.3
    Linux Mint
    Ubuntu
Gora does publish .zip artifacts for Windows users, however there is no gurantee of platform compatibility.
Gora在Ubuntu平台上开发
Gora已经在如下平台测试和认证：
    Mac OSX 10.9.2
    Linux Mint
    Ubuntu
Gora已经发布zip发行版供Windows用户，然而并不确保架构的通用性
Which Languages/Technologies do I need to know to use Gora? 使用Gora需要哪些编程语言和技能
    Gora is written in Java.
    Configuration however requires a working knowledge of syntax for JSON and XML.
    You should be able to use the command line terminal.
    You should be able to use Apache Maven from the command line.
    You should be able to edit simple flat files using a text editor.
    Gora由Java语言编写
    进行配置需要对JSON和XML语法有了解
    需要能熟练使用Terminal
    需要能够在Terminal中使用Apache Maven
    需要有文本编辑能力


Gora Modules Gora模块
Gora source code is organized in a modular architecture. The gora-core module is the main module which contains the core of the code. All other modules depend on the gora-core module. Each datastore backend in Gora resides in it's own module. The documentation for the specific module can be found at the module's documentation directory.
Gora源代码以模块化架构组织。Gora核心模块是包含代码核心的主模块。所有其他的模块依赖于主模块。以Gora为后端的每一个数据仓库都座落在它自己的仓库中。在模块文档中可以找到每一个具体模块的文档。
It is wise so start with going over the documentation for the gora-core module and then the specific data store module(s) you want to use. The following modules are currently implemented in Gora.
    gora-compiler: A page dedicated to the GoraCompiler; a critical part of the Gora workflow;
    gora-compiler-cli: A page dedicated to the GoraCompiler Command Line Interface; a utility module for working with the Gora Compiler;
    gora-shims-hadoop: Base module enabling us to use Gora with multiple versions of Hadoop in a flexible manner;
    gora-shims-hadoop-1.x: Module enabling us to use Gora with Hadoop 1.X;
    gora-shims-hadoop-2.x: Module enabling us to use Gora with Hadoop 2.X;
实现自己的数据仓库之前，过一遍Gora核心文档是非常有必要的。下面是Gora目前已经实现的模块：
    gora-compiler:用于GoraCompiler的一章，是Gora工作流中至关重要的一部分
    gora-compiler-cli:用于GoraCompiler命令控制台接口的一章，是GoraCompiler中的公共模块。
    gora-shims-hadoop:能够以灵活的方式兼容多版本的Hadoop
    gora-shims-hadoop-1.x:兼容Hadoop1.X的模块
    gora-shims-hadoop-2.x:兼容Hadoop2.X的模块
    gora-shims-hadoop-distribution: Packaging container module enabling easier dependency management whilst working with Gora Shims;
    gora-core: Module containing core functionality, AvroStore and DataFileAvroStore stores, GoraSparkEngine;
    gora-accumulo: Module for Apache Accumulo backend and AccumuloStore implementation;
    camel-gora: An Apache Camel component that allows you to work with NoSQL databases using Gora;
    gora-cassandra: Module for Apache Cassandra backend and CassandraStore implementation;
    gora-dynamodb: Module for Amazon DynamoDB backend and DynamoDBStore implementation;
    gora-hbase: Module for Apache HBase backend and HBaseStore implementation;
    gora-metamodel: Module for Apache MetaModel backend and query functionality;
    gora-mongodb: Module for MongoDB backend and MongoStore implementation;
    gora-solr: Module for Apache Solr backend and SolrStore implementation;
    gora-tutorial: The Gora LogManager tutorial;
    gora-sources-dist: Packaging module used to build and distribute Gora sources during project releases;
    gora-shims-hadoop-distribution:打包容器模块，简化依赖管理，与Gora Shims协同工作。
    gora-core:包含主要功能的模块，包含AvroStore和DataFileAcroStore仓库，GoraSparkEngine
    gora-accumulo:Apache Accumulo后端模块，AcumuloStore实现模块。
    camel-gora:一个Apache Camel组件，通过Gora来运行NoSQL
    gora-cassandra:Apacha Cassandra后端模块和Cassandra实现模块。
    gora-dynamodb:Amazon DynamoDb后端模块和DamoDBStore实现模块。
    gora-hbase:Apache HBase后端模块和HBaseStore实现模块。
    gora-metamodel:Apache MetaModel后端实现和查询功能模块。
    gora-mongodb:MongoDB后端模块和MongoStore实现模块
    gora-solr:Apache Solr后端模块和SolrStore实现模块
    gora-tutorial:Gora LogManager监管
    gora-sources-dist:在项目发行阶段，用于打包模块和发布Gora源代码
Gora Testing Gora测试
Gora currently has two testing mechanisms * JUnit Tests: These are included for every module which provides a DataStore within Gora. * Integration Tests: A custom testing suite called GoraCI (Continuous Ingestion) which stress tests Gora functionality at scale.
Gora目前有两个测试机制:JUnit Tests(在有Gora的每一个数据仓库模块中，都有JUnit Tests)和Integration Tests(一个叫GoraCI的自定义测试组件，在一定规模上对Gora功能进行压力测试)
JUnit Tests:
Unit tests in Gora are implemented using the popular JUnit framework. Each module which implements the DataStore interface similarly implements a DataStoreTestBase API which test utilities for DataStores. The DataStoreTestBase class delegates actual test execution to DataStoreTestUtil.
Gora中的单元测试使用流行的JUnit框架实现。实现数据仓库接口的每一个模块，同样实现了用来测试数据仓库功能的DataStoreTestBase API。DataStoreTestBase类将实际测试执行委托给DataStoreTestUtil。
The tests begin in a fairly trivial fashion testing functionality like datastore schema creation schema deletion, etc and continue in this manner getting progressively more complex as we begin testing some more advanced features within the Gora API. In addition to the unit tests contained within this class, the best place to look for API functionality is at the examples directories under various Gora modules. Most modules contain a /src/examples/ directory under which some example classes can be found. Specifically, there are some classes that are used for tests under gora-core/src/examples/.
测试以细致琐碎的方式来开始进行测试，如数据仓库建表和删表的测试。通过Gora API以类似的方式，进一步来测试Gora的高级特性。查看API功能的最佳是Gora模块的实例中。大部分模块中包中/src/examples/目录，里面放着一些实例。
GoraCI Integration Testsing Suite GoraCI整合测试套件
Background
Since Gora 0.5, the GoraCI suite has been part of the mainstream Gora codebase.
Credit for GoraCI can be handed to Keith Turner (Gora PMC member) for his foresight in developing GoraCI which we have now extended from gora-accumulo to the entire suite of Gora modules.
自Gora 0.5起，GoraCI已经成为主流Gora编码的一部分
GoraCI的荣誉归功于Keith Turner，用于嘉奖他在开发GoraCI的远见，由此GoraCI已经从gora-accumulo扩展到真个Gora模块套件中。
Apache Accumulo has a test suite that verifies that data is not lost at scale. This test suite is called continuous ingest.
Essentially the test runs many ingest clients that continually create linked lists containing 25 million nodes. At some point the clients are stopped and a map reduce job is run to ensure no linked list has a hole. A hole indicates data was lost.
The nodes in the linked list are random. This causes each linked list to spread across the table. Therefore if one part of a table loses data, then it will be detected by references in another part of the table.
This project is a version of the test suite written using Apache Gora [1]. Goraci has been tested against Accumulo and HBase.
Apache Accumulo 有一个测试套件用于保证数据在大规模应用中不会丢失。这个测试套件被叫做持续获取。
本质上，这测试运行很多客户端，创建了包含25,000,000个节点。在某一点，客户端被停止和一个map reduce任务运行到保证链表没有空洞。空洞的意思是数据丢失。
在链表中的节点是随机的。这引起每一个链表跨表传播。由此，如果一个表中的一部分失去了数据，那样的话，丢失的数据将会在表的另外一个部分检测出来。
这个项目是一个使用Gora测试套件版本。Goraci已经被Accumulo和HBase测试。
The Anatomy of GoraCI tests
Below is rough sketch of how data is written. For specific details look at the Generator code
    1,Write out 1 million nodes
    2,Flush the client
    3,Write out 1 million that reference previous million
    4,If this is the 25th set of 1 million nodes, then update 1st set of million to point to last
    5,goto 1
GoraCI测试的分析：
下面是数据如何被写入的草图。查看代码来看说明细节：
    1,写出百万级节点
    2,刷新节点
    3,写出百万级节点引用前面的百万级节点
    4,如果这是第25套百万级节点，然后更新第一套百万级节点指向最后的百万级节点。
    5,回到1
The key is that nodes only reference flushed nodes. Therefore a node should never reference a missing node, even if the ingest client is killed at any point in time.
When running this test suite w/ Accumulo there is a script running in parallel called the Aggitator that randomly and continuously kills server processes.
The outcome was that many data loss bugs were found in Accumulo by doing this. This test suite can also help find bugs that impact uptime and stability when run for days or weeks.
核心是节点只引用刷新的节点。那样的话，一个节点应该引用一个丢失的节点，尽管任何节点任何时间被丢失。当运行测试套件Accumulo，有一个并行运行脚本叫Aggitator随机持续杀服务器进程。结果就是：通过这样Accumulo的很多数据丢失BUG都被找到。这个测试套件通过很多天很多个星期的测试，能够帮助找到影响运行时和稳定性的BUG。
This test suite consists the following
    a few Java programs
    a little helper script to run the java programs
    a maven script to build it.
这个测试套件包含如下：
    一些Java程序
    一些帮助脚本来运行这些Java程序‘
    一个maven脚本来build
When generating data, its best to have each map task generate a multiple of 25 million. The reason for this is that circular linked list are generated every 25M. Not generating a multiple in 25M will result in some nodes in the linked list not having references. The loss of an unreferenced node can not be detected.
当生成数据的时候，最好能生成一个千万级复合体。那样的原因是：每千万级会建立一个环链表。如果不生成千万级复合体，那样的话，在链表中的某些节点将会没有引用。没有引用节点的丢失是不能被检测的。
Building GoraCI
下载GoraCI，解压后，进入目录，运行
    mvn install
The maven pom file has some profiles that attempt to make it easier to run GoraCI against different Gora backends by copying the jars you need into lib. Before packaging its important to edit gora.properties and set it correctly for your datastore. To run against Accumulo do the following.
maven相比自己手动拷jar包要方便得多了。在打包之前，需要编辑gora.properties并将它设置成你的数据仓库。
针对运行Acumulo
    vim src/main/resources/gora.properties //set Accumulo properties
    mvn package -Paccumulo-1.4
针对运行HBase
    vim src/main/resources/gora.properties //set HBase properties
    mvn package -Phbase-0.92
运行Cassandra
    vim src/main/resources/gora.properties //set Cassandra properties
    mvn package -Pcassandra-1.1.2
For other datastores mentioned in gora.properties, you will need to copy the appropriate deps into lib. Feel free to update the pom with other profiles, open a ticket or just send us a pull request.
对于其他数据仓库在gora.properties中提到，需要复制需要的依赖到lib里面。可以随意地更新pom文件里面的依赖信息。
Java Class Description
Below is a description of the Java programs
    org.apache.gora.goraci.Generator - A map only job that generates data. As stated previously, its best to generate data in multiples of 25M.
    org.apache.gora.goraci.Verify - A map reduce job that looks for holes. Look at the counts after running. REFERENCED and UNREFERENCED are ok, any UNDEFINED counts are bad. Do not run at the same time as the Generator.
    org.apache.gora.goraci.Walker - A standalong program that start following a linked list and emits timing info.
    org.apache.gora.goraci.Print - A standalone program that prints nodes in the linked list
    org.apache.gora.goraci.Delete - A standalone program that deletes a single node
    org.apache.gora.goraci.Loop - Runs generation and verify in a loop
Java类描述
下面是对Java项目的一个描述
    org.apache.gora.goraci.Generator:一个生成数据的map only job。就像先前的一样，最好能生成一个千万级的复合体。
    org.apache.gora.goraci.Verify:一个查找空洞的map reduce job。观察在运行之后的计数器。REFERENCED和UNREFERENCED都是正常的，任何UNDEFINED记数都是错误的。不要与Generator在一起运行。
    org.apache.gora.goraci.Walker:一个独立运行程序，跟随一个链表启动并记录时间信息
    org.apache.gora.goraci.Print:一个独立运行程序，输出链表中的节点
    org.apache.gora.goraci.Delete:一个独立运行程序，删除一个单一的节点
    org.apache.gora.goraci.Loop:在loop循环中运行生成和认证程序。
goraci.sh is a helper script that you can use to run the above programs. It assumes all needed jars are in the lib dir. It does not need the package name. You can just run goraci.sh Generator, below is an example.
    $ ./goraci.sh Generator
    Usage : Generator <num mappers> <num nodes>
For Gora to work, it needs a gora.properties file on the classpath and a gora-$datastore-mapping.xml mapping file on the classpath, the contents of both are datastore specific, more details can be found here [2]. You can edit the ones in src/main/resources and build the goraci-${version}-SNAPSHOT.jar with those. Alternatively remove those and put them on the classpath through some other means.
goraci.sh一个帮助脚本，可以用它来运行下面的程序。它假定所有需要的jar包都在lib文件夹中。它不需要包名。下面是运行goraci.sh的指南：
    $ ./goraci.sh Generator
    Usage : Generator <num mappers> <num nodes>
想要Gora正常运行，需要一个gora.properties文件在classpath和一个gora-$datastore-mapping.xml的mapping文件在classpath，这两个文件是依据数据仓库来自定义的。
Gora and Hadoop Gora和Hadoop
Gora uses Apache Avro which uses a Json library that Hadoop has an old version of. The two libraries jackson-core and jackson-mapper need to be updated in $HADOOP_HOME/lib and $HADOOP_HOME/share/hadoop/lib/. Currently these are updated to jackson-core-asl-1.4.2.jar and jackson-mapper-asl-1.4.2.jar. For details see HADOOP-6945.
Gora使用有Json库Apache Avro。jackson-core和jackson-mapper两个类需要添加到$HADOOP_HOME/lib和$HADOOP_HOME/share/hadoop/lib/。目前最新版本是jackson-core-asl-1.4.2.jar和jackson-mapper-asl-1.4.2.jar。
GoraCI and HBase GoraCI和HBase
To improve performance running read jobs such as the Verify step, enable scanner caching on the command line. For example:
    $ ./gorachi.sh Verify -Dhbase.client.scanner.caching=1000 \
   -Dmapred.map.tasks.speculative.execution=false verify_dir 1000
来改进运行read jobs的性能，以Verify阶段，让scanner扑捉控制台，例如：
    $ ./gorachi.sh Verify -Dhbase.client.scanner.caching=1000 \     
    -Dmapred.map.tasks.speculative.execution=false verify_dir 1000 
Dependent on how you have your Hadoop and HBase setup deployed, you may need to change the gorachi.sh script around some. Here is one suggestion that may help in the case where your Hadoop and HBase configuration are other than under the Hadoop and HBase home directories.
依赖于Hadoop和HBase是如何部署的，你可能需要修改gorachi.sh脚本。下面是推荐配置：
    iff --git a/org.apache.gora.goraci.sh b/org.apache.gora.goraci.sh
    index db1562a..31c3c94 100755
    --- a/org.apache.gora.goraci.sh
    +++ b/org.apache.gora.goraci.sh
    @@ -95,6 +95,4 @@ done
     #run it
     export HADOOP_CLASSPATH="$CLASSPATH"
     LIBJARS=`echo $HADOOP_CLASSPATH | tr : ,`
     -hadoop jar "$GORACI_HOME/lib/org.apache.gora.goraci-0.0.1-SNAPSHOT.jar" $CLASS -libjars "$LIBJARS" "$@"
     -
     -
     +CLASSPATH="${HBASE_CONF_DIR}" hadoop --config "${HADOOP_CONF_DIR} jar "$GORACI_HOME/lib/org.apache.gora.goraci-0.0.1-SNAPSHOT.jar" $CLASS -files "${HBASE_CONF_DIR}/hbase-site.xml" -libjars "$LIBJARS" "$@"

You will need to define HBASE_CONF_DIR and HADOOP_CONF_DIR before you run your goraci jobs. For example:
在运行goraci jobs之前，需要编辑HBASE_CONF_DIR和HADOOP_CONF_DIR，例如
    $ export HADOOP_CONF_DIR=/home/you/hadoop-conf
    $ export HBASE_CONF_DIR=/home/you/hbase-conf
    $ PATH=/home/you/hadoop-1.0.2/bin:$PATH ./goraci.sh Generator 1000 1000000
Concurrency 并发性
Its possible to run verification at the same time as generation. To do this supply the -c option to Generator and Verify. This will cause Genertor to create a secondary table which holds information about what verification can safely verify. Running Verify with the -c option will make it run slower because more information must be brought back to the client side for filtering purposes. The Loop program also supports the -c option, which will cause it to run verification concurrently with generation.
If verification is run at the same time as generation without the -c option, then it will inevitably fail. This is because verification mappers read different parts of the table at different times and giving an inconsistent view of the table. So one mapper may read a part of a table before a node is written, when the node is later referenced it will appear to be missing. The -c option basically filters out newer information using data written to the secondary table.
同时运行verification和generation是可能的。为了同时运行，提供-c来同时运行Gererator和Verify。这会导致Generator来创建一个secondary table来持有verification能够认证的信息。通过-c运行Verify将会将运行更慢，因为更多信息将会带到客户一端来过滤意图。通过-c运行Loop
同时运行verification和generation是可能的。为了同时运行，提供-c来同时运行Gererator和Verify。这会导致Generator来创建一个secondary table来持有verification能够认证的信息。通过-c运行Verify将会将运行更慢，因为更多信息将会带到客户一端来过滤意图。通过-c运行Loop







